---
layout: single
title: "Lecture 2 : Image Classification"
categories: CS231n
tags: [stanford, lecture, review, CS231n, Image Classification, computer vision]
toc: true
author_profile: false
sidebar:
  nav: "docs"
---
# Lecture 2 | Image Classification
<iframe width="560" height="315" src="https://www.youtube.com/embed/OoUX-nOEjG0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

***
## Assignment 1
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-2.jpg?raw=true)
   
과제는 Python 또는 Numpy를 사용합니다.   
   
## Image Classification
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-5.jpg?raw=true)
   
지난 강의에서 computer vision의 핵심 작업인 image classification에 대해 이야기했습니다.   
"image classification을 정확히 어떻게 수행합니까?"   
   
좀 더 구체적으로 말하면,   
image classification을 할때 system은 input image를 받습니다.(Ex. 고양이)   
system은 미리 결정된 categories 또는 label의 set을 인식합니다.   
(Ex. 고정된 categories label로는 비행기, 개, 고양이 또는 트럭 등이 있고 컴퓨터가 할 일은 이들 중 하나를 할당하는 것)    
이것은 machine에게 정말 매우 어려운 작업입니다.   
   
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-6.jpg?raw=true)
   
따라서, 실제로 컴퓨터가 이 image를 볼때 무엇을 보는지 깊게 생각해보면 여러분이 고양이를 볼 때 보는 고양이에 대한 holistic(전체론적) idea를 확실히 얻지 못합니다.   
그리고 컴퓨터는 실제로 image를 이 커다란 grid of numbers(숫자 격자)로 표현하고 있습니다.   
따라서 image는 800x600 pixels정도일 수 있습니다.   
컴퓨터에게 이것은 거대한 grid of numbers에 불과합니다.   
그리고 이것에서 고양이다움을 distill(추출)하는 것은 매우 어렵습니다.   
예를 들어, 수천 개의 거대한 array 또는 무엇이든 매우 다양한 numbers입니다.   
그래서 우리는 이 문제를 semantic gap이라고 부릅니다.   
고양이에 대한 이 idea 또는 고양이에 대한 이 label은 우리가 image에 할당하는 semantic label입니다.   
   
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-7.jpg?raw=true)
   
고양이의 semantic idea와 컴퓨터가 실제로 보고 있는 이러한 pixel 값 사이에는 엄청난 차이가 있습니다.   
그리고 이것은 정말 어려운 문제입니다.   
picture을 아주 작고 subtle(미묘한) 방식으로 변경하여 이 pixel grid를 완전히 변할 수 있기 때문입니다.   
예를 들어, 우리가 고양이를 잡았는데 고양이가 가만히 앉아 꿈틀거리지도 않고 근육 하나도 움직이지 않는다면 절대 일어나지 않을 일이지만 우리는 카메라를 반대쪽으로 옮겼습니다. 그러면 giant grid of numbers에서 모든 single grid, 모든 single pixel은 완전히 달라집니다.   
하지만 여전히 같은 고양이를 나타내고 있습니다.
   
따라서 우리의 algorithm은 이에 robust해야 합니다.   
   
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-8.jpg?raw=true)
   
그러나 viewpoint만이 문제가 아니라 또 다른 문제는 illumination입니다.   
scene에서는 다양한 lighting 조건이 있을 수 있습니다.   
매우 어둡고 moody한 scene 또는 매우 밝고 햇빛이 비치는 scene과 같이 어디서 고양이가 나타나든 여전히 고양이이며 우리의 algorithm은 이에 대해 robust해야 합니다.   
   
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-9.jpg?raw=true)   
   
Object도 deform(변형)될 수 있습니다.   
아마도 고양이는 밖에서 볼수 있는 동물 중 deformable한 동물에 속한다고 생각합니다.   
따라서 고양이는 정말 다양하게 다른 poses와 positions을 취할 수 있습니다.   
그리고 우리의 algorithm은 이러한 다양한 종류의 transforms에 robust해야합니다.   
   

![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-10.jpg?raw=true)  
   
또한 occlustion의 문제가 있을 수 있습니다.   
예를 들어 고양이의 일부만 볼 수 있습니다.   
예를 들어 얼굴만 보이거나 이 극단적인 예에서는 소파 쿠션 아래에서 꼬리만 볼 수 있습니다.   
  
그러나 이러한 경우 사람은 고양이라는 것을 깨닫는 것은 매우 쉽고 여전히 이러한 이미지를 고양이로 인식합니다.   
   
그리고 이것은 우리의 algorithm이 robust해야 하는 것이기도 합니다.   
   
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-11.jpg?raw=true)  
   
background clutter 문제도 있을 수 있습니다.   
고양이의 foreground object는 실제로 backgound와 appearance(모양)이 매우 유사하게 보일 수 있습니다.   
그리고 이것은 우리가 처리해야 할 또 다른 것입니다.   
   

![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-12.jpg?raw=true)  
   
class 내 variation 문제도 있습니다. 고양이다움의 한 가지 개념이 실제로는 다양한 visual appearances에 걸쳐 있다는 것입니다.   
그리고 고양이는 shapes 그리고 sizes, colors, ages가 다를 수 있습니다.   
우리의 algorithm은 이러한 모든 이 다른 variations를 처리해야합니다.   
그래서 이것은 실제로 매우 어렵습니다.   
   
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-13.jpg?raw=true)  
   
image classifier를 작성하기 위한 API가 무엇인지 생각해보면 여러분은 앉아서 이렇게 python으로 method를 작성하려고 시도 할 수 있습니다. image를 가져와 결국 이 class label을 뱉어 고양이나 개 또는 기타 등등을 말하고 싶은 곳입니다.   
이것을 할 수 있는 확실한 방법은 없습니다.   
algorithm 수업을 듣고있고 task가 숫자를 정렬하거나 convex hull을 계산하는 것 또는 심지어 RSA암호화와 같은 작업을 수행하는 것이라면 algorithm을 작성하고 필요한 모든 단계를 열거 할 수 있습니다.   
   
그러나 우리가 물체를 인식하거나 고양이 도는 image를 인식하려고 할때 이러한 물체를 인식하는 방법에 대해 직관적으로 이해할 수 있는 명확하고 명시적인 algorithm이 없습니다.   
만약 여러분이 생각해본다면, 만약 여러분이 programming 첫날이고 여러분이 앉아서 이 function을 작성해야 한다면, 대부분의 사람들이 곤경에 처할 것입니다.   

![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-14.jpg?raw=true) 
   
즉, 사람들은 다양한 동물을 인식하기 위해 일종의 고급 coding 규칙을 작성하려고 명시적으로 시도했습니다.    
그래서 우리는 지난 강의에서 Hubel과 Wiesel의 edge가 visual recognition과 관련하여 매우 중요하다는 것을 알고 있습니다.  
그러나 이것은 잘 작동하지 않는 것으로 밝혀졌습니다.   
- super brittle합니다.
- 만약 다른 object category로 다시 시작하고 싶다면 처음부터 다시 시작해야 합니다.   
   
따라서 이것은 실제로 매우 확장 가능한 접근 방식이 아닙니다.   
우리는 세계의 모든 다양한 object에 대해 훨씬 더 자연스럽게 확장되는 이러한 recognition tasks를 위한 algorithm 또는 method를 제시하고자 합니다.   

![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-15.jpg?raw=true) 
   
따라서 이 모든 것을 가능하게 하는 일종의 통찰력은 data 기반 approach 방식이라는 idea입니다.
인터넷에 나가서 많은 고양이와 많은 사슴 등처럼 많은 dataset을 수집할 것입니다.   
실제로 Google 이미지 검색과 같은 tools를 사용하여 이러한 다양한 categories의 매우 많은 예를 수집할 수 있습니다.   
이것은 실제로 나가서 이러한 dataset을 실제로 수집하는데 꽤 많은 노력이 필요하지만 다행히도 이미 사용할 수 있는 정말 좋은 고품질의 dataset이 많이 있습니다.   
그런 다음 이 dataset을 얻으면 모든 data를 수집하고 어떤 방식으로든 요약한 다음 이러한 다양한 object categories를 recognize하는 방법에 대한 지식을 요약하는 model을 내보낼 machine learning classifier를 train 합니다.   
그런 다음 마지막으로 이 training model을 사용하여 고양이와 개 등을 recognize할 수 있는 새 image에 적용합니다. 여기 API가 약간 변경되었습니다. image를 입력하고 고양이를 인식하는 단일 기능이 아닌 이 두 가지 기능이 있습니다.   
- image와 label을 입력한 다음 model을 출력하고
- 별도로 model을 입력하고 image에 대한 predictions를 수행하는 'predict'이라는 또 다른 함수가 있습니다.
그리고 이것은 지난 10년,20년 정도에 걸쳐 이 모든 것들이 정말 잘 작동하기 시작할 수 있게 한 일종의 key insight(핵심 통찰력)입니다.   
    
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-16.jpg?raw=true) 
   
따라서 이 수업은 주로 NN과 CNN, Deep Learning 등에 관한 것이지만 data 기반 접근 방식이라는 아이디어는 단순한 deep learning보다 훨씬 더 일반적입니다. 그리고 제 생각에는 이 크고 복잡한 classifier에 도달하기 전에 먼저 매우 간단한 classifier에 대해 이 process를 단계별로 살펴보는 것이 유용하다고 생각합니다.   
따라서 아마도 상상할 수 있는 가장 간단한 classifier는 우리가 nearest neigbor이라고 부르는 것입니다.   
   
algorithm은 솔직히 꽤 멍청합니다.   
따라서 training 단계에서 우리는 아무것도 하지 않고 모든 training data를 기억할 것입니다.   
그래서 이것은 매우 간단합니다.   
이제 prediction 단계에서 새로운 image를 가져오고 training에서 새 image와 가장 유사한 image를 찾으려고 시도하고 가장 유사한 image의 label을 predict합니다.  
매우 간단한 algorithm입니다.  
그러나 그것은 일종의 data기반 및 기타와 관련하여 이러한 멋진 properties(속성)을 많이 가지고 있습니다.   
   

![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-17.jpg?raw=true) 
  
그래서 좀 더 구체적으로 말하자면, machine learning에서 매우 일반적으로 사용되는 CIFAR-10이라는 이 dataset을 일종의 작은 test case로 작업하는 것을 상상할 수 있습니다.   
그리고 homework에서 이 dataset으로 작업하게 됩니다.   
따라서 CIFAR-10 데이터 세트는 비행기, 자동차, 새, 고양이 등 10가지 class를 제공합니다.   
그리고 각 10개 categories에 대해 50,000개의 training images를 제공하며, 이 10개 categories에 대략적으로 균등하게 분포되어 있습니다.   
그리고 algorithm을 테스트해야 하는 10,000개의 추가 testing images를 만듭니다.
   
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-18.jpg?raw=true) 
    
그래서 여기 CIFAR-10에서 이러한 tesing image 중 일부에 이 간단한 nearest negibor clssifier를 적용하는 예가 있습니다.    
따라서 오른쪽에 있는 이 grid에서 가장 왼쪽 열에 대해 CIFAR-10 dataset의 testing images를 제공합니다. 
이제 오른쪽에서 training images를 정렬하고 가장 유사한 training images를 보여줍니다.   
training images와 시각적으로 비슷해 보이지만 항상 정확하지는 않습니다.   
  
아마도 두 번째 행에서 우리는 test를 볼 수 있습니다.   이 이미지는 32 x 32 pixel이기 때문에 보기가 어렵습니다. 하지만 이 image는 개이고 nearest neighbor도 개입니다. 하지만 이 다음 image는 사실 사슴이나 말 또는 다른 것입니다.   
하지만 중간에 흰색 얼룩이 있고 기타 등등이 있기 때문에 시각적으로 매우 유사해 보입니다.   
따라서 이 이미지에 nearest neighbor algorithm을 적용하면 training set에서 가장 가까운 예를 찾을 수 있습니다.   
이제 가장 가까운 예는 training set에서 가져오기 때문에 label이라는 것을 알고 있습니다.  
이제 이 testing image도 개라고 간단하게 말할 것입니다.  
이 예제를 보면 잘 작동하지 않을 수 있지만 여전히 작업하기에 좋은 예제입니다.  

![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-19.jpg?raw=true) 
   
그러나 우리가 알아야 할 한 가지 세부 사항은 한 쌍의 이미지가 주어지면 실제로 어떻게 비교할 수 있습니까?   
test image를 가져와서 training images와 비교하려는 경우 실제로 비교 기능이 정확히 어떤 모양인지에 대한 다양한 선택이 있기 때문입니다.   
따라서 이전 슬라이드의 예에서는 Manhattan 거리라고도 하는 L1 거리를 사용했습니다.  
따라서 이것은 이미지를 비교하기 위한 정말 간단하고 쉬운 아이디어입니다.  
바로 이 이미지의 개별 pixel을 비교할 것입니다.
따라서 test image가 pixel 값의 작은 4 x 4 이미지라고 가정하면 test image의 왼쪽 위 픽셀에서 training image의 값을 빼고 절대값을 취합니다.  
두 이미지 사이의 해당 pixel의 차이를 얻습니다. 그런 다음 이미지의 모든 pixel에서 이 모든 것을 합산합니다.   
따라서 이것은 이미지를 비교하는 어리석은 방법이지만 때때로 합리적인 일을 합니다.  
그러나 이것은 두 이미지 간의 차이를 측정하는 매우 구체적인 방법을 제공합니다. 그리고 이 경우에는 이 두 이미지 사이에 456의 차이가 있습니다.   


![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-20.jpg?raw=true) 
   
여기 이 Nearest Neighbor classifier를 구현하기 위한 전체 Python 코드가 있습니다. NumPy에서 제공하는 이러한 vector화된 작업을 많이 사용했기 때문에 코드가 매우 짧고 간결하다는 것을 알 수 있습니다.  
   
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-21.jpg?raw=true) 
   
여기에서 우리가 이전에 이야기한 이 training 기능이 매우 간단하다는 것을 알 수 있습니다. nearest neighbor의 경우 training을 기억하기만 하면 됩니다. 여기서 할 일이 많지 않습니다.
  
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-22.jpg?raw=true) 

그리고 이제 test time에 우리는 image를 가져온 다음 이 L1 distance function을 사용하여 test image를 이러한 각 training 예제와 비교하고 training set에서 가장 유사한 예제를 찾을 것입니다. NumPy에서 이러한 vector화된 작업을 활용하여 단 한 줄 또는 두 줄의 Python 코드로 실제로 이 작업을 수행할 수 있음을 알 수 있습니다.  
그래서 이것은 첫 번째 과제에서 연습하게 될 것입니다.  
  
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-23.jpg?raw=true) 
  
이제 이 간단한 classifier에 대한 몇 가지 질문입니다.  
첫째, training set에 N개의 예가 있는 경우 training가 testing가 얼마나 빠를 것으로 예상할 수 있습니까?   
    
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-24.jpg?raw=true) 
     
우리가 실제로 아무것도 할 필요가 없기 때문에 training은 아마도 일정할 것입니다.  
data를 암기하기만 하면 됩니다.  
pointer만 복사하는 경우에는 dataset의 크기에 관계없이 일정한 시간이 됩니다.  
그러나 이제 test time에 이 comparison stop를 수행하고 test image를 dataset의 각 N training 예제와 비교해야 합니다.   그리고 이것은 실제로 매우 느립니다.  
   
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-25.jpg?raw=true) 

그래서, 당신이 그것에 대해 생각한다면 이것은 실제로 다소 backwards입니다.  
실제로 우리는 classifier가 training time에는 느리고 testing ime에는 빠르기를 원하기 때문입니다.  
왜냐하면 classifiter가 어딘가의 data center에서 train을 받고 classifier를 정말 좋게 만들기 위해 training에 많은 계산을 할 수 있다고 상상할 수 있기 때문입니다.  
그러나 test time에 classifier를 배포할 때 휴대 전화, 브라우저 또는 기타 저전력 장치에서 실행되기를 원하고 classifier의 test time 성능이 매우 빠르기를 원합니다.    
따라서 이러한 관점에서 볼 때 이 Nearest Neighbor algorithm은 실제로는 약간 backward입니다. 그리고 CNN과 다른 유형의 parametric model로 이동하면 이와 반대가 된다는 것을 알게 될 것입니다.  
training time에 많은 computing을 사용하지만 testing time에는 상당히 빠릅니다.  
  
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-26.jpg?raw=true)  
  
문제는 이 Nearest Neighbor algorithm을 실제로 적용할때 정확히 어떤 모습일까?  
그래서. 여기에 우리는 Nearest Neighbor classifier의 decision regions라고 불리는 것을 그렸습니다.  
따라서 여기에서 training set은 2차원 평면의 이러한 점으로 구성되며 점의 색상은 categories나 해당 class label을 나타냅니다.  
그래서 corner에 5개의 class와 일부 파란색 class가 있고 오른쪽 상단 cornar에 보라색 class가 있습니다.  
이제 이 전체 평면의 각 pixel에 대해 이러한 교육 data에서 가장 가까운 예를 계산한 다음 class label에 해당하는 background point에 색상을 지정했습니다.  
따라서 이 Nearest Neighbor classifier가 공간을 분할하고 주변 point에 색상을 지정했습니다.  
따라서 이 Nearest Neighbor classifier가 공간을 분하랗고 주변 point에 따라 공간을 색칠하는 것임을 알 수 있습니다.  
  
그러나 이 classifier는 그다지 좋지 않을 수 있습니다.  
그리고 이 그림을 보면 Nearest Neighbor classifier로 나타날 수 있는 몇 가지 문제를 볼 수 있습니다.  
우선, 이 중앙 영역에는 실제로 대부분 녹색 점이 포함되어 있지만 중간에 작은 노란색 점이 하나 있습니다.  
그러나 우리는 단지 nearest neighbor로 보고 있기 때문에 이 녹색 cluster의 중간에 작은 노란색 island가 나타납니다.   
그리고 그것은 아마도 그렇게 좋지 않을 것입니다.  
아마도 그 포인트는 실제로 녹색이었을 것입니다.  
그런 다음 유사하게 우리는 녹색 영역이 파란색 영역으로 밀리는 것과 같은 일종의 fingers로 봅니다 .  
잡음이 있거나 가짜일 수 있는 한 지점의 존재로 인해 다시 나타납니다.  
  

![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-27.jpg?raw=true)  
  
따라서 이런 종류의 동기(앞ppt내용)는 k-NN(k-nearest neighbors)이라고 하는 이 algorithm의 약간의 generalization에 동기를 부여합니다.  
따라서 단일 nearest neighbor을 찾는 대신 distance metric에 따라 조금 더 멋진 work을 수행하고 nearest neighbor인 K를 찾은 다음 각 neighbor 중에서 투표를 합니다.  
그런 다음 neighbors 사이에서 다수결을 predict합니다.  
이를 수행하는 약간 더 복잡한 방법을 상상할 수 있습니다.  
아마도 당신은 distance에 weight를 두고 투표할 것입니다.  
  
그래서 여기서 우리는 이 K=1 Nearest Neighbor를 사용하여 정확히 동일한 set of points를 보여주었고, 가운데와 오른쪽에는 K=3과 K=5도 있었습니다.  
그리고 K=3으로 이동하면 녹색 cluster 중간에 있는 가짜 노란색 점이 더 이상 해당 영역 근처의 점을 노란색으로 분류하지 않는 것을 볼 수 있습니다.  
이제 중간에 있는 전체 녹색 부분이 모두 녹색으로 분류됩니다.  

또한 이 다수결 투표로 인해 빨간색과 파란색 영역의 fingers가 부드러워지기 시작하는 것을 볼 수 있습니다.  
그런 다음 K=5 사례로 이동하면 파란색과 빨간색 영역 사이의 이러한 decision boundaries가 매우 매끄럽고 매우 좋아집니다.    
따라서 일반적으로 Nearest Neighbor을 사용하는 경우 거의 항상 1보다 큰 K 값을 사용하려고 합니다.  
이는 decision boundaries를 부드럽게 하고 더 나은 결과로 이어지는 경향이 있기 때문입니다.  
  
[Q/A]
흰색영역은 무엇을 다룹니까?  
  
흰색영역은 k-NN 중 majority가 없는 영역입니다.  
약간 더 멋진 일을 하고 추측을 하거나 대다수의 승자 중에서 무작위로 선택하는 것을 상상할 수 있지만 이 간단한 예에서는 해당 지점에 Nearest neighbor이 없음을 나타내기 위해 흰색으로 표시했습니다.  
  
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-29.jpg?raw=true) 
  
우리가 computer vision에 대해 생각할 때마다 저는 여러 다른 관점 사이를 왔다 갔다 하는 것이 정말 유용하다고 생각합니다.  
하나는 평면의 고차원 지점에 대한 아이디어이고 다른 하나는 실제로 구체적인 이미지를 보는 것입니다.  
image의 pixel은 실제로 이러한 image를 고차원으로 생각할 수 있게 해주기 때문입니다.   
벡터. 그리고 이 두 가지 다른 관점 사이에서 앞뒤로 탁구를 치는 것은 일종의 유용합니다.  
그래서, 일종의 이 k-NN을 가지고 이미지로 돌아가면 실제로 좋지 않다는 것을 알 수 있습니다.  
여기서 나는 nearest neighbor에 따라 이미지가 실제로 올바르게 분류되는지 또는 잘못 분류되는지 빨간색과 녹색으로 색칠했습니다.  
그리고 실제로 그다지 좋지 않다는 것을 알 수 있습니다.  
그러나 아마도 우리가 더 큰 K 값을 사용했다면 이것은 실제로 상위 3개 또는 상위 5개 또는 전체 행 사이에서 투표하는 것을 포함할 것입니다.  
그리고 이런 방식으로 Neighbor을 검색할 때 우리가 보는 노이즈 중 일부에 대해 훨씬 더 강력해질 것이라고 상상할 수 있습니다.  

![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-30.jpg?raw=true) 
  
따라서 k-NN algorithm으로 작업할 때 선택할 수 있는 또 다른 선택은 서로 다른 지점을 비교하는 방법을 정확하게 결정하는 것입니다.  
지금까지의 예에서 우리는 pixel 사이의 절대값의 합을 취하는 이 L1 거리에 대해 이야기했습니다.  
그러나 또 다른 일반적인 선택은 제곱합의 제곱근을 취하고 이를 거리로 취하는 L2 또는 Euclidean distance입니다.
서로 다른 거리 metric을 선택하는 것은 공간에서 예상할 수 있는 기본 기하학 또는 topology에 대해 서로 다른 가정을 하기 때문에 실제로 매우 흥미로운 주제입니다. 그래서, 이 L1 거리, 이것 아래, 이것은 실제로 L1 거리에 따른 원이고 이것은 원점 주위에 이 사각형 모양을 형성합니다.  
이 사각형의 각 점은 L1에 따라 원점에서 등거리에 있는 반면 L2 또는 Educlidean distance를 사용하면 이 원은 친숙한 원이며 예상한 것과 같습니다.  
특히 이 두 metric 사이에서 지적해야 할 한 가지 흥미로운 점은 L1 거리가 선택한 좌표계에 따라 달라진다는 것입니다.   따라서 좌표계를 회전하면 점 사이의 L1 거리가 실제로 변경됩니다.   
  
L2 거리에서 좌표 프레임을 변경하는 것은 중요하지 않지만 좌표 프레임이 무엇이든 상관없습니다.  
입력 기능이 있고 벡터의 개별 항목이 작업에 중요한 의미가 있는 경우 어떻게든 L1이 더 자연스럽게 맞을 수 있습니다.   
그러나 그것이 어떤 공간에 있는 일반적인 벡터이고 다른 요소 중 어떤 것이 실제로 무엇을 의미하는지 모른다면 L2가 약간 더 자연스러울 수 있습니다.  
여기서 또 다른 요점은 서로 다른 거리 메트릭을 사용하여 k-nearest neighbor classifier를 이미지뿐만 아니라 벡터뿐만 아니라 다양한 유형의 데이터로 실제로 일반화할 수 있다는 것입니다.   
예를 들어 pieces of text을 분류하고 싶다고 가정하면 k-NN을 사용하기 위해 해야 할 유일한 일은 두 단락 또는 두 문장 사이의 거리를 측정할 수 있는 거리 함수를 지정하는 것입니다.  
따라서 다른 거리 metric을 지정하기만 하면 실제로 이 algorithm을 기본적으로 모든 유형의 데이터에 매우 일반적으로 적용할 수 있습니다.  
일종의 간단한 algorithm이지만 일반적으로 새로운 문제를 볼 때 가장 먼저 시도하는 것은 아주 좋은 것입니다.  
  
따라서 다른 거리 메트릭을 선택하면 실제로 기하학적으로 발생하는 일에 대해 생각하는 것도 흥미로울 것입니다.

![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-31.jpg?raw=true)  
  
여기서 우리는 L1 또는 Manhattan distance를 사용하여 왼쪽에 동일한 점 set을 볼 수 있고, 오른쪽에는 친숙한 L2 또는 Euclidean distance를 사용하여 볼 수 있습니다.  
그리고 이러한 decision boundaries의 모양이 실제로 두 지표 사이에서 꽤 많이 변하는 것을 볼 수 있습니다.  
따라서 L1을 볼 때 이러한 decision boundaries는 좌표축을 따르는 경향이 있습니다.  
L1은 좌표계 선택에 따라 달라지기 때문입니다.  
L2 종류는 좌표축에 대해 실제로 신경쓰지 않는 곳에서 자연스럽게 떨어지는 경계를 설정합니다.  
  
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-32.jpg?raw=true)  
  
제 고백은 제가 여러분에게 보여드린 각 예제가 실제로 제가 구축한 이 대화형 웹 데모에서 가져온 것이며 여기에서 k-NN classifier를 직접 가지고 놀 수 있다는 것입니다.  

![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-34.jpg?raw=true)  
  
그렇다면 문제는 실제로 이 algorithm을 실제로 사용하려고 하면 몇 가지 선택을 해야 한다는 것입니다.  
우리는 K의 다른 값을 선택하는 것에 대해 이야기했습니다.  
우리는 다양한 distance metric을 선택하는 것에 대해 이야기했습니다.   
   
question은 problem와 data에 대해 실제로 어떻게 이러한 선택을 하느냐가 됩니다.  
따라서 K 및 distance metric과 같은 이러한 선택은 hyperparameter라고 합니다.  
training에서 learn할 필요가 없기 때문에, 대신 algorithm에 대한 선택사항이며 data에서 직접 learn할 수 있는 방법은 없습니다.  
  
그렇다면 문제는 이러한 것들을 실제로 어떻게 설정하느냐입니다. 그리고 그것들은 매우 문제 의존적임이 밝혀졌습니다.  
그리고 대부분의 사람들이 하는 간단한 일은 데이터와 문제에 대해 서로 다른 hyperparameter 값을 시도하고 어떤 것이 가장 잘 작동하는지 알아내는 것입니다.  
  
[Q/A]  
따라서 질문은 L2 거리를 사용하는 것보다 L1 거리가 더 나은 위치는 어디입니까?  
  
나는 그것이 주로 문제에 따라 다르다고 생각합니다. 어떤 경우에 하나가 다른 것보다 낫다고 생각하는지 말하기는 어렵습니다.
하지만 저는 L1이 이런 종류의 좌표 dependency를 가지고 있기 때문에 실제로는 데이터의 좌표계에 의존한다고 생각합니다. 벡터가 있고 벡터의 개별 요소가 의미가 있다는 것을 알고 있다면 말입니다.  
예를 들어 어떤 이유로 직원을 분류한 다음 해당 벡터의 다른 요소가 직원의 다른 기능이나 측면에 해당하는 경우가 있습니다.
월급이나 회사에서 일한 햇수 등이요.  
따라서 개별 요소가 실제로 어떤 의미를 가질 때 L1을 사용하는 것이 조금 더 의미가 있다고 생각합니다.  
  
그러나 일반적으로 다시 말하지만 이것은 hyperparameter이며 실제로 문제와 데이터에 따라 다르므로 최선의 대답은 두 가지를 모두 시도하고 무엇이 더 잘 작동하는지 확인하는 것입니다.  
  
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-36.jpg?raw=true)  
  
hyperparameter의 다른 값을 시도하고 가장 잘 작동하는 것을 보는 아이디어조차도 여기에는 다양한 선택이 있습니다.  
hyperparameter를 시도하고 무엇이 가장 잘 작동하는지 확인한다는 것은 정확히 무엇을 의미합니까?  
  
여러분이 생각할 수 있는 첫 번째 아이디어는 training data에서 최고의 정확도 또는 최고의 성능을 제공하는 hyperparameter를 선택하는 것입니다.  
**절대 이렇게 해서는 안 됩니다.**  
NN classifier의 구체적인 경우,  
예를 들어 K=1로 설정하면 training data를 항상 완벽하게 분류할 것입니다.  
따라서 이 전략을 사용하면 항상 K=1을 선택하게 되지만 이전 예제에서 보았듯이 실제로는 K를 더 큰 값으로 설정하면 일부 training data를 잘못 분류할 수 있습니다.  
사실 training data에 없는 지점에서 더 나은 성능을 이끌어냅니다.  
  
그리고 궁극적으로 machine learning에서 우리는 training data를 맞추는 데 관심이 없습니다.  
우리는 classifier 또는 method이 training 후 보이지 않는 데이터에서 어떻게 수행되는지에 대해 정말로 관심이 있습니다.   그래서 이것은 끔찍한 생각입니다. 이것을 하지 마십시오.  
  
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-38.jpg?raw=true)  
  
따라서 여러분이 생각할 수 있는 또 다른 아이디어는 전체 dataset을 가져와 일부 training data와 일부 test data로 분할하는 것입니다.  
이제 training data에 대해 다양한 hyperparameter를 선택하여 algorithm을 training한 다음 trained된 classifier를 test data에 적용하고 이제 test data에서 최고의 성능을 발휘할 수 있는 hyperparameter set을 선택하겠습니다.   
이것은 아마도 더 합리적인 전략처럼 보이지만 사실 이것은 또한 끔찍한 생각이며 **절대 이렇게 해서는 안 됩니다.**  
  
다시 말하지만 machine learning의 요점은 우리 algorithm이 어떻게 수행되는지 알고 싶어하기 때문입니다.  
따라서 test set의 요점은 wild에서 나오는 보이지 않는 데이터에 대해 우리 방법이 어떻게 작동하는지에 대한 estimate를 제공하는 것입니다.  
그리고 우리가 서로 다른 hyperparameter로 많은 다른 algorithm을 교육하는 이 전략을 사용하고 test data에서 가장 잘 작동하는 것을 선택하면 algorithm을 유발한 올바른 hyperparameter set을 선택했을 수 있습니다. 그러나 이제 이 test set에 대한 우리의 성능은 더 이상 보이지 않는 새로운 데이터의 성능을 나타내지 않을 것입니다.  
  
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-39.jpg?raw=true)  
   
훨씬 더 일반적인 것은 실제로 데이터를 세 가지 다른 세트로 분할하는 것입니다.  
대부분의 데이터를 training set으로 분할한 다음  validation set and test set를 생성합니다.
이제 우리가 일반적으로 하는 일은 training set에서 다양한 hyperparameter 선택으로 algorithm을 train하고 validation set에서 평가한 다음 validation set에서 가장 잘 수행되는 hyperparameter set을 선택하는 것입니다.  
이제 모든 개발을 완료하고 모든 디버깅을 완료한 후 validation set에서 가장 성능이 좋은 classifier를 가져와서 test set에서 한 번 실행합니다.  
그리고 이제 그것이 당신의 논문에 들어가는 숫자이고, 당신의 보고서에 들어가는 숫자이고, 실제로 당신의 algorithm이 보이지 않는 데이터에 대해 어떻게 작동하는지 알려주는 숫자입니다.   
validation data과 test data를 매우 엄격하게 분리하는 것이 실제로 매우 중요합니다.  
   
예를 들어 연구 논문을 작성할 때 우리는 일반적으로 마지막 순간에만 test set를 만집니다.  
그래서, 제가 논문을 쓸 때, 저는 우리가 여기서 dishonest하지도 않고 unfair 숫자를 보고하지도 않는다는 것을 확실히 하기 위해 마감 일주일 정도 전에 제 문제에 대한 test set만 만지는 경향이 있습니다.  
  
따라서 이것은 실제로 매우 중요하며 test data를 확실히 통제하기를 원합니다.  
  
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-40.jpg?raw=true)  
  
따라서 hyperparameter를 설정하는 또 다른 전략은 cross validation이라고 합니다.  
그리고 이것은 작은 data set에서 조금 더 일반적으로 사용되며 deep learning에서는 많이 사용되지 않습니다.  
여기서 아이디어는 test data를 가져오거나 평소와 같이 data set을 가져오고 마지막에 사용할 test set을 보유하고 이제 나머지 데이터에 대해 single training 및 validation partition으로 분할하는 대신 training data를 여러 다른 folds로 분할할 수 있습니다.  
  
이제 이러한 방식으로 validation set이 될 fold를 선택하는 과정을 cycled했습니다.  
이제 이 예에서는 5차 fold cross validation를 사용하므로 처음 4차에서 한 set의 hyperparameter로 algorithm을 train하고 4차에서 performance을 evaluate한 다음 이제 1차에서 algorithem을 다시 training합니다.  
fold 1,2,3,4,5에서 evaluate하고 모든 다른 fold를 cycled합니다.  
  
그리고 이런 방식으로 수행하면 어떤 hyperparameter가 더 강력하게 수행될 것인지에 대해 훨씬 더 높은 확신을 갖게 됩니다.  
따라서 이것은 사용하기에 일종의 gold standard이지만, 실제로 우리가 large model을 training할 때 deep learning에서 training은 계산 cost이 매우 많이 들기 때문에 실제로는 많이 사용되지 않습니다.  

[Q/A] training set과 validation set의 차이점은 무엇입니까?   
   
k-NN classifier에 대해 생각해보면 training set은  label을 기억하는 label이 있는 이 image set입니다.   
이제 이미지를 분류하기 위해 이미지를 가져와 training data의 각 element와 비교한 다음 가장 가까운 training point에서 label을 transfer합니다.  
이제 우리의 algorithm은 training set의 모든 것을 memorize할 것입니다.  
  
이제 validation set의 각 element를 가져와 training data의 각 element와 비교한 다음 이를 사용하여 validation set에서 적용 시 classifier의 accuracy를 결정합니다.  
  
이것이 training과 validation의 차이점입니다.  
algorithm이 training set의 label을 볼 수 있는 경우,  
그러나 validation set의 경우 algorithm이 label에 직접 access할 수 없습니다.  
  
우리는 algorithm이 얼마나 잘 작동하는지 확인하기 위해 validation set의 label만 사용합니다.  
  
   

[Q/A]질문? [질문하는 학생]
test set이 wild data를 대표하지 않을 수 있는지 여부?   

이것은 확실히 실제로 문제가 될 수 있습니다.  
여기서 기본 통계적 가정은 데이터가 모두 독립적이고 동일하게 분포되어 있으므로 모든 데이터 포인트가 동일한 기본 확률 분포에서 가져와야 한다는 것입니다.  
물론 실제로는 항상 그런 것은 아니며 test set이 실제 상황을 완전히 대표하지 못하는 경우가 있을 수 있습니다.  
따라서 이것은 dataset creators와 dataset curators는 생각해야 합니다.
   
하지만 예를 들어 data set을 생성할 때 제가 하는 한 가지는 data를 수집하는 데 정확히 동일한 방법론을 사용하여 한 번에 모든 데이터를 수집하는 것입니다.  
그런 다음 나중에 가서 train과 test 사이에 무작위로 분할합니다.  
  
여기에서 당신을 망칠 수 있는 한 가지는 아마도 당신이 시간이 지남에 따라 데이터를 수집하고 처음에 수집한 초기 데이터를 training data로 만들고 나중에 수집한 데이터를 test data로 만든 다음 실제로 문제를 일으킬 수 있는 이러한 변화에 부딪힐 수 있습니다.  
그러나 이 patition이 전체 set of data points에서 무작위인 한 실제로 이 문제를 완화하기 위해 시도하는 방법입니다.  
  
따라서 이 cross validation procedure를 거치면 다음과 같은 그래프로 끝납니다.  
  
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-41.jpg?raw=true)  
   
그래서 여기 X축에는 어떤 문제에 대한 k-NN claasifier에 대한 K 값을 표시하고 있으며, 이제 Y축에는 서로 다른 K 값에 대한 일부 dataset에 대한 classifier의 accuracy를 보여주고 있습니다.  
이 경우 데이터에 대해 5 fold cross validation을 수행했으므로 K의 각 값에 대해 이 algorithm이 얼마나 잘 수행되는지에 대한 5가지 다른 예가 있습니다.  
그리고 실제로 algorithm에 대해 더 좋거나 더 나쁜 일부 test set을 갖는 것에 대한 질문으로 돌아가서 K fold cross validation을 사용하는 것이 아마도 그것을 quantify(정량화)하는 데 도움이 되는 한 가지 방법일 것입니다.  
그리고 그 안에서 우리는 이 algorithm이 서로 다른 validation folds에서 수행되는 방식의 차이를 볼 수 있습니다.  
그리고 그것은 당신에게 최고가 무엇인지 뿐만 아니라 그 성능의 분포가 무엇인지에 대한 sense을 제공합니다.  
  
따라서 ML model을 교육할 때마다 다음과 같은 plots을 만들게 됩니다.  
여기서 plots은 accuray 또는 performance를 hyperparameters의 function으로 보여줍니다.  
그런 다음 가서 model을 선택하거나 validation set에서 최상의 성능을 발휘하는 hyperparameter set입니다.  
  
따라서 여기에서 K=7 정도가 아마도 이 문제에 대해 가장 잘 작동한다는 것을 알 수 있습니다.
  
  
![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-42.jpg?raw=true)  
   
따라서 이미지의 k-NN classifier는 실제로 실제로 거의 사용되지 않습니다.  
왜냐하면, 우리가 이야기한 이 모든 문제들 때문입니다.  
- 한 가지 문제는 테스트 시간에 매우 느리다는 것입니다. 이는 우리가 이전에 이야기한 것과는 반대입니다.
- 유클리드 거리 또는 L1 거리와 같은 것들이 실제로 이미지 사이의 거리를 측정하는 좋은 방법이 아니라는 것입니다.  
이러한 종류의 벡터 거리 함수는 이미지 간의 지각적 유사성과 잘 일치하지 않습니다.  
   
그래서 이 예에서 우리는 소녀의 왼쪽에 이 이미지가 있고 오른쪽에 그녀의 입을 막고 있는 3개의 다른 왜곡된 이미지를 구성했습니다.   
실제로 몇 pixel 아래로 이동했습니다. 또는 전체 이미지를 파란색으로 착색했습니다.  
그리고 실제로 원본과 boxed, 원본과 shuffled, 착색 원본 사이의 Euclidean distance를 계산하면 모두 동일한 L2 거리를 갖습니다.  
L2 거리가 이미지 사이의 이러한 perceptional distance를 capturing하는 데 실제로 잘 작동하지 않는다는 느낌을 주기 때문에 그다지 좋지 않을 수 있습니다.  

![](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-43.jpg?raw=true)
  
k-NN classifer와 관련된 또 다른 문제는 우리가 차원의 curse(저주)라고 부르는 것과 관련이 있습니다.   
따라서 k-NN classifier에 대한 이 관점을 다시 생각해 보면 각 training data points 주위에 페인트를 떨어뜨리고 이를 사용하여 공간을 분할하는 것과 같습니다.  
즉, k-NN classifier가 제대로 작동할 것으로 기대한다면 공간을 상당히 조밀하게 커버하기 위한 training examples가 필요합니다.  
그렇지 않으면 Nearest Neighbor이 실제로 꽤 멀리 떨어져 있을 수 있으며 실제로 test points과 매우 유사하지 않을 수 있습니다.  
그리고 문제는 실제로 공간을 densely 덮는다는 것은 문제의 차원에서 기하급수적으로 많은 training examples가 필요하다는 것을 의미합니다.   
기하급수적인 성장은 항상 좋지 않습니다. 기본적으로 이 고차원 공간에서 이 pixel 공간을 densely 덮을 수 있는 충분한 이미지를 얻지 못할 것입니다.    
k-NN을 사용할 때 염두에 두어야 할 또 다른 사항입니다.  
요약하자면 우리는 k-NN을 사용하여 이 이미지 분류 아이디어를 소개합니다.  
  
[Q/A]
녹색 점과 파란색 점은 무엇입니까?  
  
여기 points로 표시되는 training samples가 있습니다.  
point의 색상은 이 traning sample의 point categories를 나타낼 수 있습니다.  
따라서 우리가 1차원에 있는 경우 공간을 densely 덮기 위해 4개의 training samples만 필요할 수 있지만 2차원으로 이동하면 이제 4 곱하기 4는 이 공간을 densely 덮기 위해 16개의 training samples이 필요합니다.  
그리고 더 많은 차원으로 이동하면 공간을 densely 커버하는 데 필요한 training samples의 수가 차원에 따라 기하급수적으로 증가합니다.  
2차원에서 우리는 이런 종류의 우스꽝스러운 곡선 모양을 가질 수도 있고 다른 차원 공간에서 일종의 임의의 다양한 label을 가질 수도 있습니다.  
k-NN algorithm은 이러한 기본 manifolds에 대해 실제로 어떤 가정도 하지 않기 때문에 제대로 수행할 수 있는 유일한 방법은 작업할 training points의 상당히 dense sample이 있는 경우입니다.  
이것은 일종의 k-NN에 대한 개요이며 이를 실제로 구현하고 첫 번째 과제에서 이미지에 사용해 볼 수 있는 기회를 얻게 될 것입니다.  
  
![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-44.jpg?raw=true)
   
## Linear Classification
![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-45.jpg?raw=true)
    
다음으로 이야기하고 싶은 것은 Linear Classification입니다.   
Linear Classification는 다시 말하지만 매우 간단한 learning algorithm이지만 이것은 매우 중요해지고 전체 Neural Network(NN)과 전체 convolution network를 구축하는 데 도움이 될 것입니다.   
  
![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-46.jpg?raw=true)
   
따라서 사람들이 신경망으로 작업할 때 자주 이야기하는 비유 중 하나는 신경망을 레고 블록과 같은 것으로 생각한다는 것입니다.   
다양한 종류의 신경망 구성 요소를 가질 수 있고 이러한 구성 요소를 함께 결합하여 이렇게 크고 다른 컨볼루션 네트워크 타워를 구축할 수 있습니다.   
다양한 유형의 딥 러닝 애플리케이션에서 볼 수 있는 가장 기본적인 빌딩 블록 중 하나는 이 Linear classification입니다.
따라서 Linear classification에서 일어나는 일을 잘 이해하는 것이 실제로 매우 중요하다고 생각합니다.  
이것들은 전체 신경망에 아주 잘 일반화될 것이기 때문입니다.   
  

![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-48.jpg?raw=true)
   
신경망의 이러한 모듈식 특성에 대한 또 다른 예는 약간의 미리 보기와 같이 이미지 캡션에 대한 우리 연구실의 일부 연구에서 나옵니다.   
그래서 여기서 설정은 이미지를 입력한 다음 이미지를 설명하는 설명문을 출력하는 것입니다.   
그리고 이런 종류의 작동 방식은 이미지를 보는 하나의 컨볼루션 신경망과 언어에 대해 알고 있는 순환 신경망이 있다는 것입니다.   
그리고 우리는 이 두 조각을 레고 블록처럼 함께 붙일 수 있고 모든 것을 함께 훈련시킬 수 있고 결국에는 사소한 일을 할 수 있는 꽤 멋진 시스템으로 끝납니다.   
그리고 우리는 수업을 진행하면서 이 모델의 세부 사항을 통해 작업할 것입니다. 하지만 이것은 여러분에게 이러한 심층 신경망이 레고와 비슷하고 이 Linear classification이 가장 기본적인 거대한 network의 building blocks와 같다는 느낌을 줍니다.   
  
![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-49.jpg?raw=true)
  
CIFAR-10에는 이러한 50,000개의 train examples가 있으며 각 이미지는 32 x 32 pixel과 3개의 색상 채널입니다.  
Linear Classification에서 우리는 k-최근접 이웃과는 약간 다른 접근 방식을 취할 것입니다.
   
  
![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-50.jpg?raw=true)
  
따라서 linear classifier는 우리가 parametric model이라고 부르는 것의 가장 간단한 예 중 하나입니다.  
이제 parametric model에는 실제로 두 가지 구성 요소가 있습니다.  
아마도 왼쪽에 있는 고양이의 이미지를 가져올 것입니다.  
그리고 이것은 우리가 일반적으로 입력 데이터에 대해 X로 쓰는 것입니다.  
또한 일반적으로 W라고 하는 set of parameters 혹은 weights를 가져옵니다.(문헌에 따라 다릅니다)   
   
이제 우리는 데이터 X와 parameter W를 모두 받는 함수를 작성할 것입니다.  
그러면 CIFAR-의 10개 categories 각각에 해당하는 점수를 설명하는 10개의 숫자가 출력됩니다.  
고양이에 대한 더 큰 점수와 마찬가지로 입력 X가 고양이일 확률이 더 높다는 해석이 있습니다.  
   
따라서 k-NN 설정에는 parameter이 없었습니다.  
대신 the whole training data, the whole training set를 유지하고 test time에 사용했습니다.    
그러나 이제  parametric approach방식에서 training data에 대한 지식을 요약하고 모든 지식을 이러한 parameter W에 집어넣을 것입니다.  
그리고 이제 test time에 더 이상 실제 training data가 필요하지 않으므로 버릴 수 있습니다.  
테스트 시에는 이러한 parameter W만 필요합니다.  
  
따라서 이제 모델이 더 효율적이고 실제로 휴대폰과 같은 작은 장치에서 실행될 수 있습니다.  
따라서 딥 러닝의 전체 이야기는 이 함수 F에 대한 올바른 구조로 다가오고 있습니다.  
서로 다른 복잡한 방식으로 가중치와 데이터를 결합하는 방법에 대해 서로 다른 기능적 형식을 작성하는 것을 상상할 수 있으며 이는 서로 다른 네트워크 아키텍처에 해당할 수 있습니다.  
그러나이 두 가지를 결합하는 가장 간단한 예는 아마도 그것들을 곱하는 것입니다.  
그리고 이것은 Linear classifier입니다.  
  
![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-53.jpg?raw=true)
    
여기서 X의 F, W는 W 곱하기 X와 같습니다.  
아마도 당신이 상상할 수 있는 가장 간단한 방정식일 것입니다. 그래서 여기에서 이러한 것들의 크기를 풀면 이미지가 32 x 32 x 3 값이었음을 기억합니다.  
그런 다음 해당 값을 가져와 3,072개의 항목이 있는 긴 열 벡터로 확장할 것입니다. 그리고 이제 우리는 10개의 class scores로 끝내고 싶습니다.    
우리는 이 이미지에 대해 10개의 categories 각각에 대한 score를 제공하는 10개의 숫자로 끝내고 싶습니다.  
이것은 이제 행렬 W가 10 x 3072가 되어야 한다는 것을 의미합니다.  
따라서 이 두 가지를 곱하면 단일 열 벡터 10 x 1이 되어 10개의 class score를 제공합니다.  
  
우리는 종종 training data와 상호 작용하지 않는 10개 elements의 constant vector인 bias 항을 추가하고 대신 일부 class에 대한 일종의 데이터 independent preferences를 제공합니다.   
예를 들어 data set의 균형이 맞지 않고 개보다 고양이가 더 많다면 고양이에 해당하는 bias가 다른 요소보다 높을 것이라고 상상할 수 있습니다.   
   
![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-54.jpg?raw=true)
   
따라서 이 함수가 수행하는 작업을 그림으로 생각한다면 이 그림에서 간단한 이미지의 왼쪽에 2x2 이미지가 있는 예가 있으므로 총 4개의 pixels가 있습니다.  
따라서 linear classifier가 작동하는 방식은 이 2x2 이미지를 가져와 4개의 elements가 있는 열 벡터로 확장하는 것입니다.  
이제 이 예에서는 cat, dog, ship에 10을 맞출 수 없기 때문에 이제 weight matix이 4x3이 될 것이므로 4개의 pixels과 3개의 classes가 있습니다.    
이제 다시 각 categories에 대해 데이터 independent bias 항을 제공하는 3 element bias vector가 있습니다.  
이제 고양이 score가 이미지의 pixel과 이 bias 항과 함께 추가된 weight matrix의 이 행 사이의 enter product가 될 것임을 알 수 있습니다.   
   
따라서 이런 식으로 보면 linear classifier를 거의 template matching approach로 이해할 수 있습니다.     
여기서 이 행렬의 각 행은 이미지의 일부 template에 해당합니다.   
그리고 이제 행렬의 행과 이미지의 pixel을 제공하는 열 사이에 enter product 또는 dot product을 입력합니다.  
이 dot product를 계산하면 class에 대한 이 templated와 이미지의 pixel 간의 유사성이 제공됩니다.  
그런 다음 bias는 다시 각 class에 대한 이 data independence scaling offset을 제공합니다.  
   
![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-56.jpg?raw=true)
     
template matching의 이러한 관점에서 linear classification에 대해 생각하면 실제로 해당 weight matrix의 행을 가져와 이미지로 다시 풀고 실제로 해당 template을 이미지로 visualize할 수 있습니다.  
이를 통해 데이터를 이해하기 위해 linear classifier이 실제로 무엇을 하는지 알 수 있습니다.   
따라서 이 예에서는 이미지에 대해 linear classifier를 trained했습니다.  
이제 하단에는 CIFAR-10의 10개 categories 각각에 해당하는 trained된 weight matrix의 행이 무엇인지 visualizing하고 있습니다.   
그리고 이런 식으로 우리는 이 이미지에서 무슨 일이 일어나고 있는지에 대한 sense을 얻습니다.   
  
예를 들어, 왼쪽 하단 왼쪽에는 비행기 class에 대한 template이 있습니다.  
파란색 blob과 같은 것으로 구성되어 있습니다.  
중간에 이런 종류의 blob이 있고 배경에는 파란색일 수 있습니다.  
비행기에 대한 이 linear classifier가 아마도 파란색 stuff과 얼룩덜룩한 stuff을 찾고 있고 이러한 features으로 인해 classifier가 더 비행기 같다라는 것이라는 의미입니다.    
  
또는 이 자동차 예를 보면 가운데에 빨간색 얼룩이 있고 상단에 파란색 얼룩이 있는 것을 볼 수 있습니다. 흐릿한 앞 유리일 수도 있습니다.  
그러나 이것은 약간 이상합니다.  이것은 실제로 자동차처럼 보이지 않습니다.  
따라서 문제는 linear classifier가 각 class에 대해 하나의 template만 training한다는 것입니다.
따라서 해당 class가 표시될 수 있는 방식에 일종의 변형이 있는 경우 모든 다양한 변형, 모든 다른 모양을 평균화하고 각 category를 인식하는 데 하나의 단일 template만 사용하려고 합니다.  
  
우리는 또한 이것을 말 classifier에서 꽤 명시적으로 볼 수 있습니다.  
그래서 말 classifier에서 우리는 말이 보통 잔디 위에 있기 때문에 바닥에 녹색 stuff를 봅니다.  
그리고 주의 깊게 보면 말은 실제로 두 개의 머리를 가지고 있는 것처럼 보입니다. 양쪽에 머리가 하나씩 있습니다.  
그리고 나는 머리가 두 개인 말을 본 적이 없습니다. 그러나 linear classification는 category당 하나의 template만 학습할 수 있기 때문에 최선을 다하고 있습니다.    
그리고 우리가 신경망과 더 복잡한 모델로 이동함에 따라 category당 단일 template을 학습하는 이러한 제한이 더 이상 없기 때문에 훨씬 더 나은 accuracy를 달성할 수 있습니다.  
  
![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-57.jpg?raw=true)
  
linear classifier의 또 다른 관점은 이미지를 점과 고차원 공간으로 보는 아이디어로 돌아가는 것입니다.  
그리고 각각의 이미지가 이 고차원 공간의 점과 같다고 상상할 수 있습니다.  
  
이제 linear classifier는 이러한 linear decision boundaries를 설정하여 하나의 category와 나머지 categories 사이의 분리하는 linear를 그리려고 합니다.   
   
따라서 아마도 왼쪽 상단에서 우리는 비행기의 이러한 training 예를 볼 수 있고 training 과정 전반에 걸쳐 linear classifier가 가서 이 파란색 선을 그려 비행기 등급을 나머지 모든 class에서 한 줄로 분리하려고 시도할 것입니다.  
training 과정 중에 이러한 선이 무작위로 시작하여 데이터를 적절하게 분리하기 위해 제자리에 고정되는 것을 보면 실제로 재미있습니다.  
하지만 이런 방식으로 linear classification에 대해 생각할 때, 이 고차원적 관점에서 linear classification에서 발생할 수 있는 문제 중 일부가 무엇인지 다시 볼 수 있습니다.  
그리고 linear classifier가 완전히 실패하는 data set의 예를 구성하는 것은 그리 어렵지 않습니다.  
   
![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-58.jpg?raw=true)
    
여기 왼쪽에 있는 한 가지 예는 두 가지 categories의 dataset가 있고 모두 다소 인공적일 수 있지만 dateset에 파란색과 빨간색의 두 가지 categories가 있다고 가정합니다.    
그리고 파란색 category는 이미지의 pixel 수이며 0보다 큰 홀수입니다.  
0보다 큰 pixel 수가 짝수인 모든 항목을 빨간색 category로 분류하려고 합니다.  
따라서 실제로 이동하여 이러한 다양한 decision regions이 평면에서 어떻게 보이는지 그리면 홀수의 pixel이 있는 파란색 class가 평면의 이 두 사분면이 되고 반대쪽 두 사분면이 될 것임을 알 수 있습니다.  
이제 파란색과 빨간색을 구분하기 위해 단일 선형 선을 그릴 수 있는 방법이 없습니다.  
따라서 이것은 선형 분류기가 실제로 어려움을 겪는 예가 될 것입니다. 그리고 이것은 결국 그렇게 인공적인 것이 아닐 수도 있습니다.  
픽셀을 세는 대신 실제로 이미지에 있는 동물이나 사람의 수가 홀수인지 짝수인지 세려고 할 수도 있습니다. 
따라서 홀수와 짝수를 분리하는 이런 종류의 parity 문제는 linear classifier가 전통적으로 정말 어려움을 겪고 있는 것입니다.  
linear classifier가 실제로 어려움을 겪는 다른 상황은 다중 모드 상황입니다.  
여기 오른쪽에 파란색 범주에는 파란색 범주가 있는 세 개의 다른 island이 있고 나머지는 다른 범주입니다.  
  
따라서 이전 예에서 보았던 말과 같은 것은 실제로 실제로 일어날 수 있는 것입니다.  
왼쪽을 바라보는 말의 픽셀 공간에 하나의 섬이 있고 오른쪽을 바라보는 또 다른 말의 섬이 있습니다. 이제 이 두 개의 고립된 데이터 island 사이에 단일 선형 boundry를 그리는 좋은 방법이 없습니다.  
따라서 서로 다른 공간 영역에 나타날 수 있는 하나의 클래스와 같은 다중 모드 데이터가 있는 곳은 linear classifier가 어려움을 겪을 수 있는 또 다른 장소입니다.  
따라서 linear classifier에는 많은 문제가 있지만 매우 간단한 알고리즘이며 매우 훌륭하고 해석하기 쉽고 이해하기 쉽습니다.
따라서 실제로 첫 번째 숙제에서 이러한 것들을 구현하게 될 것입니다.  
    
![summary](https://github.com/jungyun24/jungyun24.github.io/blob/master/_image/cs231n/l2/1f84d1b08f70417dcaf15463d867e54eBPtTa74reD6uEqs2-59.jpg?raw=true)   
    
이 시점에서 우리는 linear classifier에 해당하는 함수형이 무엇인지에 대해 이야기했습니다. 
그리고 우리는 이 행렬 벡터 곱셈의 기능적 형태가 template 일치 및 데이터의 각 category에 대한 learning a single template 아이디어와 일치한다는 것을 확인했습니다. 그런 다음 이 trained matrix가 있으면 이를 사용하여 새로운 training matrix에 대한 점수를 얻을 수 있습니다.  
그러나 우리가 말하지 않은 것은 실제로 dataset에 적합한 W를 선택하는 방법입니다.  
우리는 방금 기능적 형태가 무엇인지 그리고 이것으로 무슨 일이 일어나고 있는지에 대해 이야기했습니다. 그래서 그것은 우리가 다음 시간에 정말로 집중할 것입니다.  
   
-끝-